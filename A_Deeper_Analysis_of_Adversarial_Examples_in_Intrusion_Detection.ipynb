{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Deeper Analysis of Adversarial Examples in Intrusion Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmWYMvp_A4_M",
        "colab_type": "text"
      },
      "source": [
        "## A Deeper Analysis of Adversarial Examples in Intrusion Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYr9FidOIOLW",
        "colab_type": "text"
      },
      "source": [
        "## Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzFYfiRmIJri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time as time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "!pip install adversarial-robustness-toolbox >/dev/null\n",
        "from art.attacks.evasion import FastGradientMethod, BasicIterativeMethod, DeepFool, SaliencyMapMethod, CarliniL2Method, CarliniLInfMethod\n",
        "from art.classifiers import PyTorchClassifier\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.display.max_columns = 150\n",
        "pd.options.display.max_rows = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-KDXZmOI_fJ",
        "colab_type": "text"
      },
      "source": [
        "## NSL-KDD Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzucpNF9JJq0",
        "colab_type": "text"
      },
      "source": [
        "### Dowloading and importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRL6avqAI58a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Downloading and extracting the dataset if it doesn't exist\n",
        "!if [ ! -d \"./NSL-KDD\" ]; then wget http://205.174.165.80/CICDataset/NSL-KDD/Dataset/NSL-KDD.zip; mkdir NSL-KDD; unzip NSL-KDD.zip -d NSL-KDD; fi\n",
        "    \n",
        "#Importing the training and testing datasets from .CSV to Pandas DataFrames\n",
        "features = ['1 Duration', '2 Protocol-type : ', '3 Service : ', '4 Flag : ', '5 Src-bytes', '6 Dst-bytes', '7 Land', '8 Wrong-fragment', '9 Urgent', '10 Hot', '11 Num-failed-logins', '12 Logged-in', '13 Num-compromised', '14 Root-shell', '15 Su-attempted', '16 Num-root', '17 Num-file-creations', '18 Num-shells', '19 Num-access-files', '20 Num-outbound-cmds', '21 Is-host-login', '22 Is-guest-login', '23 Count', '24 Srv-count', '25 Serror-rate', '26 Srv-serror-rate', '27 Rerror-rate', '28 Srv-rerror-rate', '29 Same-srv-rate', '30 Diff-srv-rate', '31 Srv-diff-host-rate', '32 Dst-host-count', '33 Dst-host-srv-count', '34 Dst-host-same-srv-rate', '35 Dst-host-diff-srv-rate', '36 Dst-host-same-src-port-rate', '37 Dst-host-srv-diff-host-rate', '38 Dst-host-serror-rate', '39 Dst-host-srv-serror-rate', '40 Dst-host-rerror-rate', '41 Dst-host-srv-rerror-rate', '42 Attack_type', '43 Difficulty']\n",
        "df_training = pd.read_csv('./NSL-KDD/KDDTrain+.txt', names=features)\n",
        "df_testing = pd.read_csv('./NSL-KDD/KDDTest+.txt', names=features)\n",
        "# Stack the training and test sets\n",
        "data = pd.concat([df_training, df_testing], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZEBgnPxJW6j",
        "colab_type": "text"
      },
      "source": [
        "### Removing the unsed features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckN7Jw_OJdTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop the last column (which might be the difficulty, so it's useless)\n",
        "data.drop('43 Difficulty', inplace=True, axis=1)\n",
        "# Drop the 19th column wich is full of 0, so has std=0. which causes issues for the normalization\n",
        "data.drop('20 Num-outbound-cmds', inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhAWbR2BJsms",
        "colab_type": "text"
      },
      "source": [
        "### Transforming the problem into binary clasification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps4JWj7hJ0j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the nominal attribute \"Attack type\" into binary (0 : normal / 1 : attack)\n",
        "labels = (data['42 Attack_type'] != 'normal').astype('int64')\n",
        "data['42 Labels'] = labels\n",
        "data.drop('42 Attack_type', inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY72JDkDJ8fZ",
        "colab_type": "text"
      },
      "source": [
        "### One Hot Encoding the categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqLzA5xzKE6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One Hot Encode the 3 first nominal attributes and drop them\n",
        "for i in ['4 Flag : ', '3 Service : ', '2 Protocol-type : ']:\n",
        "    # Create the One Hot Encode DataFrame\n",
        "    dum = pd.get_dummies(data[i])\n",
        "    # Insert into the dataset DataFrame by Series\n",
        "    for column_name in list(dum.columns):\n",
        "        data.insert(1, str(i)+column_name, dum[column_name])\n",
        "        data[str(i)+column_name] = data[str(i)+column_name].astype('int64')\n",
        "    # Drop the old attribute's column\n",
        "    data.drop(i, inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofSVMjNSKYPp",
        "colab_type": "text"
      },
      "source": [
        "### Spliting the training and test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zJU5gyGKcEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split training and test sets\n",
        "df_training = data[:df_training.shape[0]]    \n",
        "df_testing = data[df_training.shape[0]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsjviwI7KIc1",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing the data using Min-Max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuDGFomBKP1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Min-Max normalization on the non binary features\n",
        "for i in ['1 Duration', '5 Src-bytes', '6 Dst-bytes', '8 Wrong-fragment', '9 Urgent', '10 Hot', '11 Num-failed-logins', '13 Num-compromised', '15 Su-attempted', '16 Num-root', '17 Num-file-creations', '18 Num-shells', '19 Num-access-files', '23 Count', '24 Srv-count', '25 Serror-rate', '26 Srv-serror-rate', '27 Rerror-rate', '28 Srv-rerror-rate', '29 Same-srv-rate', '30 Diff-srv-rate', '31 Srv-diff-host-rate', '32 Dst-host-count', '33 Dst-host-srv-count', '34 Dst-host-same-srv-rate', '35 Dst-host-diff-srv-rate', '36 Dst-host-same-src-port-rate', '37 Dst-host-srv-diff-host-rate', '38 Dst-host-serror-rate', '39 Dst-host-srv-serror-rate', '40 Dst-host-rerror-rate', '41 Dst-host-srv-rerror-rate']:\n",
        "    # The min and max are only computed from the training set\n",
        "    min = df_training[i].min()\n",
        "    max = df_training[i].max()\n",
        "    df_training[i] = ((df_training[i] - min) / (max - min)) \n",
        "    df_testing[i] = ((df_testing[i] - min) / (max - min)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stMNv0C-Lw5V",
        "colab_type": "text"
      },
      "source": [
        "### Converting the training and testing set into NumPy **array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pa51jRiL39N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get NumPy arrays from DataFrames\n",
        "nd_training = df_training.values\n",
        "nd_testing = df_testing.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhhq7PClMAZN",
        "colab_type": "text"
      },
      "source": [
        "### Extracting the labels and making copies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8qQrNKVMExe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating arguments (x) from lables (y)\n",
        "x_train = nd_training[:, :-1]\n",
        "y_train = nd_training[:, -1]\n",
        "x_test = nd_testing[:, :-1]\n",
        "y_test = nd_testing[:, -1]\n",
        "\n",
        "# Make a copy of the data set as NumPy arrays\n",
        "x_train_np = x_train.copy()\n",
        "y_train_np = y_train.copy()\n",
        "x_test_np = x_test.copy()\n",
        "y_test_np = y_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaHQn0KBMJgz",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX40cQe1MQ0H",
        "colab_type": "text"
      },
      "source": [
        "### Convert the training and testing set into PyTorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrpWo4BcMIRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert from numpy array to torch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWXR5RaO40r",
        "colab_type": "text"
      },
      "source": [
        "### Define a neural network with 2 ReLU hidden layers and a Softmax output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UizcDVxqO3_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    ''' A basic neural network model '''\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()         #python2 : super(MLP, self).__init__()\n",
        "        #defining the network's operations\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
        "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "        self.fc3 = nn.Linear(hidden_size[1], output_size)\n",
        "\n",
        "    def forward(self, x, softmax=False): \n",
        "        a = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x.float())))))\n",
        "        if softmax:\n",
        "            y_pred = F.softmax(a, dim=1)\n",
        "        else:\n",
        "            y_pred = a\n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNRHSdafPN-q",
        "colab_type": "text"
      },
      "source": [
        "### Define a function to compute the accuracy of the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e0BvbFUPVtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_predictions(predictions, real):\n",
        "    ''' Evaluates the accuracy of the predictions'''\n",
        "    n_correct = torch.eq(predictions, real).sum().item()\n",
        "    accuracy = n_correct / len(predictions) * 100\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOwiOdMyPcMF",
        "colab_type": "text"
      },
      "source": [
        "### Define a function that prints the models perfomance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1aB1Z_3PjMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stat_model(model, x_train, y_train, x_test, y_test):\n",
        "    ''' Prints statistics about the model performances on the dataset'''\n",
        "    _, predictions_train = model(x_train, softmax=True).max(dim=1)\n",
        "    #_, predictions_train = model(x_train).max(dim=1)\n",
        "    accuracy_train = evaluate_predictions(predictions=predictions_train.long(), real=y_train)\n",
        "\n",
        "    _, predictions_test = model(x_test, softmax=True).max(dim=1)\n",
        "    #_, predictions_test = model(x_test).max(dim=1)\n",
        "    accuracy_test = evaluate_predictions(predictions=predictions_test.long(), real=y_test)\n",
        "    \n",
        "    print(\"Final Training Accuracy: {0:.4f}%\\nFinal Testing Accuracy : {1:.4f}%\"\n",
        "          .format(accuracy_train, accuracy_test))\n",
        "    # Move the tensors back to CPU\n",
        "    label_test_final = y_test.cpu().numpy()\n",
        "    predictions_test_final = predictions_test.cpu().numpy()\n",
        "    report = classification_report(label_test_final, predictions_test_final)\n",
        "    print(\"Classification Report :\")\n",
        "    print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBO2uZVxPmEJ",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHA_5qGMPk45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set fixed seeds for reproducibility\n",
        "# Even with fixed seeds, we noticed that trained models may be different. (https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Initialising the model\n",
        "input_size=x_train.shape[1]\n",
        "hidden_size=[256,256]\n",
        "output_size=2\n",
        "model = Network(input_size, hidden_size, output_size)\n",
        "\n",
        "# Setting device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Training on : {}\".format(device))\n",
        "\n",
        "# Transfering model and data to GPU\n",
        "model = model.to(device)\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# Setting the Loss function and Adam learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.01\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Variables to store the best performences (weights and accuracy)\n",
        "best_model_weights = copy.deepcopy(model.state_dict())\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# DataFrame for the learning curve plot\n",
        "trace = pd.DataFrame(columns=['epoch', 'train_acc', 'test_acc'])\n",
        "\n",
        "# Iterrating on the dataset\n",
        "since = time.time()\n",
        "for epoch in range(500+1):\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train) \n",
        "    # torch.max(dim=1) returns the maximum value of each line AND its index\n",
        "    _, predictions = y_pred.max(dim=1)\n",
        "    # Compute accuracy\n",
        "    accuracy_train = evaluate_predictions(predictions=predictions.long(), real=y_train)\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Testing model on the test set\n",
        "    if epoch%10 == 0:\n",
        "        _, predictions_test = model(x_test, softmax=True).max(dim=1)\n",
        "        accuracy_test = evaluate_predictions(predictions=predictions_test.long(), real=y_test)\n",
        "        # Keep track of the accuracies for the learning curve\n",
        "        trace = trace.append([{'epoch':epoch,\n",
        "                                'train_acc':accuracy_train,\n",
        "                                'test_acc':accuracy_test}])\n",
        "        # Save the best model's accuracy and parameters\n",
        "        if accuracy_test > best_accuracy:\n",
        "            best_accuracy = accuracy_test\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        # Displap statistics\n",
        "        if epoch%100 == 0:\n",
        "            time_elapsed = time.time() - since\n",
        "            print(\"epoch: {0:4d} | loss: {1:.4f} | Train accuracy: {2:.4f}% | Test accuracy: {3:.4f}% [{4:.4f}%] | Running for : {5:.0f}m {6:.0f}s\"\n",
        "                  .format(epoch,\n",
        "                          loss,\n",
        "                          accuracy_train,\n",
        "                          accuracy_test,\n",
        "                          best_accuracy,\n",
        "                          time_elapsed // 60,\n",
        "                          time_elapsed % 60))\n",
        "\n",
        "    # Zero all gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "# Compute the training time\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IqslUHHRjfl",
        "colab_type": "text"
      },
      "source": [
        "### Display the learning curve and the model performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIfrxK78RobZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw the learning curve\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.scatter(data=trace, x='epoch', y='train_acc', c=\"b\", s=5)\n",
        "plt.scatter(data=trace, x='epoch', y='test_acc', c=\"r\", s=5)\n",
        "plt.plot(trace['epoch'], trace['train_acc'], c=\"b\")\n",
        "plt.plot(trace['epoch'], trace['test_acc'], c=\"r\")\n",
        "plt.ylim((0, 100))\n",
        "plt.yticks(np.arange(0, 100, 5))\n",
        "plt.grid()\n",
        "\n",
        "# Loading the best weights and displaying the best model's performances\n",
        "model.load_state_dict(best_model_weights)\n",
        "stat_model(model, x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUUZasB2CSx3",
        "colab_type": "text"
      },
      "source": [
        "### Saving/Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opZmOHI7CYfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"./model.pytorch\")\n",
        "model.load_state_dict(torch.load(\"./model.pytorch\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2yiT2olRN9F",
        "colab_type": "text"
      },
      "source": [
        "## Adversarial Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0sHszZR39m",
        "colab_type": "text"
      },
      "source": [
        "### Define a table for statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPX4nhBSQ-4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adv_feat_stats = pd.DataFrame(index=df_training.columns[:-1])\n",
        "\n",
        "adv_results = pd.DataFrame(index=['Accuracy', \n",
        "                                  'Mean perturbed features   [Mean L0]', \n",
        "                                  'Max perturbed features    [Max  L0]', \n",
        "                                  'Mean Euclidiant distance  [Mean L2]', \n",
        "                                  'Max Euclidiant distance   [Max  L2]', \n",
        "                                  'Mean Maximum perturbation [Mean Li]', \n",
        "                                  'Max Maximum perturbation  [Max  Li]'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ziF5wllSF40",
        "colab_type": "text"
      },
      "source": [
        "### Define a function to compute Lp norms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stOmGKtsSOgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adv_norms(x_test_cpu, adversarial_examples_cpu):\n",
        "    mean_l0 = np.mean(np.sum(x_test_cpu != adversarial_examples_cpu, axis=1))\n",
        "    max_l0 = np.max(np.sum(x_test_cpu != adversarial_examples_cpu, axis=1))\n",
        "    mean_l2 = np.mean(np.sum(np.power(x_test_cpu - adversarial_examples_cpu, 2), axis=1, keepdims=True))\n",
        "    max_l2 = np.max(np.sum(np.power(x_test_cpu - adversarial_examples_cpu, 2), axis=1, keepdims=True))\n",
        "    mean_li = np.mean(np.max(np.abs(x_test_cpu - adversarial_examples_cpu), axis=1, keepdims=True))\n",
        "    max_li = np.max(np.max(np.abs(x_test_cpu - adversarial_examples_cpu), axis=1, keepdims=True))\n",
        "    return [mean_l0, max_l0, mean_l2, max_l2, mean_li, max_li]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhcKW1IMSVVz",
        "colab_type": "text"
      },
      "source": [
        "### Extracte the attacks samples and copy them in the device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yMQum6QSc2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_examples = df_testing[df_testing['42 Labels'] == 1].values\n",
        "x_test = torch.from_numpy((positive_examples[:, :-1])).float()\n",
        "y_test = torch.from_numpy((positive_examples[:, -1])).float()\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFB2IcSk0d",
        "colab_type": "text"
      },
      "source": [
        "### Clean Data\n",
        "The model performance on untouched attack samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofkaYZoYSpwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, predictions_clean = model(x_test, softmax=True).max(dim=1)\n",
        "accuracy_clean = evaluate_predictions(predictions=predictions_clean.long(), real=y_test)\n",
        "\n",
        "attack='Clean'\n",
        "\n",
        "# Exporting the clean examples in a .xlsx file\n",
        "excel_writer = pd.ExcelWriter(\"adversarial_examples.xlsx\", engine='openpyxl') \n",
        "xlsx_export = pd.DataFrame(np.hstack((x_test.cpu().numpy(),y_test.cpu().numpy().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "adv_results[attack] = [accuracy_clean] + adv_norms(x_test_cpu, x_test_cpu)\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L4sJYvbTF-Y",
        "colab_type": "text"
      },
      "source": [
        "### Fast Gradient Sign Method\n",
        "*Goodfellow et al. (2015) \"Explaining and Harnessing Adversarial Examplse\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DKFhHZnTkyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = FastGradientMethod(classifier,\n",
        "                                         norm=np.inf,\n",
        "                                         eps=0.1,\n",
        "                                         targeted=False,\n",
        "                                         num_random_init=0,\n",
        "                                         batch_size=128,\n",
        "                                         )\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'FGSM'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)\n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 10e-6).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_fgsm = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btrkyYmgZAOw",
        "colab_type": "text"
      },
      "source": [
        "### Basic Iterative Method\n",
        "\n",
        "*Kurakin et al. (2016) \"Adversarial examples in the physical world\"*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jV3uIUxZEBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = BasicIterativeMethod(classifier, \n",
        "                                           eps=0.1, \n",
        "                                           eps_step=0.001,\n",
        "                                           max_iter=100, \n",
        "                                           targeted=False, \n",
        "                                           batch_size=128)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'BIM'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)\n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 10e-6).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_bim = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mw0zPmqZGzL",
        "colab_type": "text"
      },
      "source": [
        "### DeepFool\n",
        "*Moosavi-Dezfooli et al (2016) \"DeepFool: a simple and accurate method to fool deep neural networks\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axf8gApiZPKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = DeepFool(classifier, \n",
        "                               max_iter=100, \n",
        "                               epsilon=1e-6, \n",
        "                               nb_grads=10, \n",
        "                               batch_size=128) #default max_iter=100, epsilon=1e-6, nb_grads=10, batch_size=1)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'DF'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)            \n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 10e-6).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_df = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpjvuo_LZTGh",
        "colab_type": "text"
      },
      "source": [
        "### Carlini & Wagner L2 Attack\n",
        "*Carlini et al. (2017) \"Towards Evaluating the Robustness of Neural Networks\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G0CIFD1Zcex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = CarliniL2Method(classifier,\n",
        "                                      confidence=0.0,\n",
        "                                      targeted=False,\n",
        "                                      learning_rate=0.01,\n",
        "                                      binary_search_steps=10,\n",
        "                                      max_iter=10,\n",
        "                                      initial_const=0.01,\n",
        "                                      max_halving=5,\n",
        "                                      max_doubling=5,\n",
        "                                      batch_size=128)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "# The transformation to tanh space introduce some small perturbation, we remove it to get the exact statistics\n",
        "adversarial_examples = pd.DataFrame(adversarial_examples)\n",
        "adversarial_examples[(np.abs(adversarial_examples_cpu - x_test_cpu) < 10e-6)] = x_test_cpu\n",
        "adversarial_examples = adversarial_examples.values\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'CW2'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)  \n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 0).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_cw2 = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkaPGkUuZ_Jh",
        "colab_type": "text"
      },
      "source": [
        "### Carlini & Wagner L∞ Attack\n",
        "*Carlini et al. (2017) \"Towards Evaluating the Robustness of Neural Networks\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2akJNaUaB1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = CarliniLInfMethod(classifier,\n",
        "                                        confidence=0.0,\n",
        "                                        targeted=False,\n",
        "                                        learning_rate=0.01,\n",
        "                                        max_iter=10,\n",
        "                                        max_halving=5,\n",
        "                                        max_doubling=5,\n",
        "                                        eps=0.3,\n",
        "                                        batch_size=128)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "# The transformation to tanh space introduce some small perturbation, we remove it to get the exact statistics\n",
        "adversarial_examples = pd.DataFrame(adversarial_examples)\n",
        "adversarial_examples[(np.abs(adversarial_examples_cpu - x_test_cpu) < 10e-6)] = x_test_cpu\n",
        "adversarial_examples = adversarial_examples.values\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'CW∞'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)  \n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 0).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_cwi = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLttu7yNaaf5",
        "colab_type": "text"
      },
      "source": [
        "### Carlini & Wagner L0 Attack\n",
        "*Carlini et al. (2017) \"Towards Evaluating the Robustness of Neural Networks\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6q0aZhJahfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WARNING : Since the warm-start is not implemented on this version, the generation take time.\n",
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size)\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = CarliniL0Method(classifier,\n",
        "                                      confidence=0.0,\n",
        "                                      targeted=False,\n",
        "                                      learning_rate=0.01,\n",
        "                                      binary_search_steps=10,\n",
        "                                      max_iter=10,\n",
        "                                      initial_const=0.01,\n",
        "                                      max_halving=5,\n",
        "                                      max_doubling=5,\n",
        "                                      batch_size=128)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'CW0'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)  \n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 0).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_cw0 = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKPSEN8Cn9HW",
        "colab_type": "text"
      },
      "source": [
        "### Jacobian-based Saliency Map Attack\n",
        "*Papernot et al. (2016) The limitations of deep learning in adversarial settings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3c4_vosoP1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the PyTorch wrapper\n",
        "classifier = PyTorchClassifier(model=model, loss=criterion, optimizer=optimizer, input_shape=input_size, nb_classes=output_size, clip_values=(0,1))\n",
        "# Creating the adversarial examples crafter\n",
        "adversarial_crafter = SaliencyMapMethod(classifier,\n",
        "                                        theta = 0.1,\n",
        "                                        gamma = 1.0,\n",
        "                                        batch_size=1)\n",
        "# Generating the adversarial examples\n",
        "adversarial_examples = adversarial_crafter.generate(x=x_test.cpu())#, y=np.ones(x_test.shape[0]))\n",
        "\n",
        "adversarial_examples = torch.from_numpy(adversarial_examples).float()\n",
        "adversarial_examples = adversarial_examples.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "stat_model(model, x_test, y_test, adversarial_examples, y_test)\n",
        "\n",
        "adversarial_examples_cpu = np.array(adversarial_examples.cpu())\n",
        "x_test_cpu = np.array(x_test.cpu())\n",
        "\n",
        "_, predictions_adv = model(adversarial_examples, softmax=True).max(dim=1)\n",
        "accuracy_adv = evaluate_predictions(predictions=predictions_adv.long(), real=y_test)\n",
        "attack = 'JSMA'\n",
        "adv_results[attack] = [accuracy_adv] + adv_norms(x_test_cpu, adversarial_examples_cpu)  \n",
        "\n",
        "# Exporting the adversarial examples in a .xlsx file\n",
        "xlsx_export = pd.DataFrame(np.hstack((adversarial_examples_cpu,y_test.cpu().reshape(y_test.shape[0], 1))), columns=data.columns)\n",
        "xlsx_export['Adversarial prediction'] = predictions_adv.cpu().numpy().reshape(y_test.shape[0], 1)\n",
        "xlsx_export.to_excel(excel_writer, sheet_name=attack)\n",
        "\n",
        "# Saving the statistics in a table\n",
        "perturbation = np.abs(adversarial_examples_cpu - x_test_cpu)\n",
        "adv_feat_stats[attack] = ((perturbation > 0).sum(axis=0) / perturbation.shape[0]) * 100\n",
        "adversaria_examples_jsma = pd.DataFrame(adversarial_examples_cpu.copy(), columns=df_training.columns[:-1])\n",
        "\n",
        "\n",
        "print(adv_results[attack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN5gJi-s8Y9S",
        "colab_type": "text"
      },
      "source": [
        "### Writing the adversarial examples .xlsx file\n",
        "This file can be used to analyse the adversarial examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLtI8jdSQmTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "excel_writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SIBuBF-Trk4",
        "colab_type": "text"
      },
      "source": [
        "### Statistics of the different attack methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeDUTbFK_78Q",
        "colab_type": "text"
      },
      "source": [
        "#### Detection rate and Lp distances on clean and adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRdjKOgLTlkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adv_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9GT6e6zAPn6",
        "colab_type": "text"
      },
      "source": [
        "#### Percentage of adversarial examples that perturb each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRjCuGWNU9TM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adv_feat_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hh0jUNkAiXI",
        "colab_type": "text"
      },
      "source": [
        "#### Heat map of the previous table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU2zUIi9bJsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(5,30))\n",
        "sb.heatmap(adv_feat_stats.round(decimals=2), annot=True, cmap='YlOrRd', fmt='g')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys7PQddP9AlN",
        "colab_type": "text"
      },
      "source": [
        "## Heat map of the correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce0yZXak9EHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_features =['1 Duration', '5 Src-bytes', '6 Dst-bytes', '12 Logged-in', '23 Count', '24 Srv-count', '25 Serror-rate', '26 Srv-serror-rate', '27 Rerror-rate', '28 Srv-rerror-rate', '29 Same-srv-rate', '30 Diff-srv-rate', '31 Srv-diff-host-rate', '32 Dst-host-count', '33 Dst-host-srv-count', '34 Dst-host-same-srv-rate', '35 Dst-host-diff-srv-rate', '36 Dst-host-same-src-port-rate', '37 Dst-host-srv-diff-host-rate', '38 Dst-host-serror-rate', '39 Dst-host-srv-serror-rate', '40 Dst-host-rerror-rate', '41 Dst-host-srv-rerror-rate']\n",
        "corr = data[numeric_features].corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "im = ax.imshow(corr, cmap='RdBu', vmin=-1, vmax=1)\n",
        "\n",
        "ax.figure.colorbar(im)\n",
        "plt.yticks(np.arange(0, len(corr.columns), 1), corr.columns)\n",
        "plt.xticks(np.arange(0, len(corr.columns), 1), corr.columns, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "for i in range(len(numeric_features)):\n",
        "    for j in range(len(numeric_features)):\n",
        "        text = ax.text(j, i, corr.iloc[i, j].round(decimals=2), ha=\"center\", va=\"center\", color=\"w\")\n",
        "        \n",
        "plt.savefig('correlation_matrix.eps', format='eps',bbox_inches='tight')  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MOi0Rhuqb-3",
        "colab_type": "text"
      },
      "source": [
        "## Carlini&Wagner L0 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AvLuYhZjHAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MIT License\n",
        "#\n",
        "# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2018\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
        "# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n",
        "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n",
        "# persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n",
        "# Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n",
        "# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
        "# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\"\"\"\n",
        "This module implements the L0 optimized attacks `CarliniL0Method` of Carlini and Wagner\n",
        "(2016). These attacks are among the most effective white-box attacks and should be used among the primary attacks to\n",
        "evaluate potential defences. A major difference with respect to the original implementation\n",
        "(https://github.com/carlini/nn_robust_attacks) is that this implementation uses line search in the optimization of the\n",
        "attack objective.\n",
        "\n",
        "| Paper link: https://arxiv.org/abs/1608.04644\n",
        "\"\"\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from art.config import ART_NUMPY_DTYPE\n",
        "from art.estimators.estimator import BaseEstimator\n",
        "from art.estimators.classification.classifier import ClassGradientsMixin\n",
        "from art.attacks.attack import EvasionAttack\n",
        "from art.utils import compute_success, get_labels_np_array, tanh_to_original, original_to_tanh\n",
        "from art.utils import check_and_transform_label_format\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CarliniL0Method(EvasionAttack):\n",
        "    \"\"\"\n",
        "    The L_0 distance metric is non-differentiable and therefore is ill-suited for standard gradient descent.\n",
        "    Instead, we use an iterative algorithm that, in each iteration, identifies some pixels that don’t have much effect\n",
        "    on the classifier output and then fixes those pixels, so their value will never be changed. The set of fixed pixels\n",
        "    grows in each iteration until we have, by process of elimination, identified a minimal (but possibly not minimum)\n",
        "    subset of pixels that can be modified to generate an adversarial example. In each iteration, we use our L_2 attack\n",
        "    to identify which pixels are unimportant.\n",
        "\n",
        "    | Paper link: https://arxiv.org/abs/1608.04644\n",
        "    \"\"\"\n",
        "\n",
        "    attack_params = EvasionAttack.attack_params + [\n",
        "        \"confidence\",\n",
        "        \"targeted\",\n",
        "        \"learning_rate\",\n",
        "        \"max_iter\",\n",
        "        \"binary_search_steps\",\n",
        "        \"initial_const\",\n",
        "        \"mask\",\n",
        "        \"warm_start\",\n",
        "        \"max_halving\",\n",
        "        \"max_doubling\",\n",
        "        \"batch_size\",\n",
        "    ]\n",
        "\n",
        "    _estimator_requirements = (BaseEstimator, ClassGradientsMixin)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifier,\n",
        "        confidence=0.0,\n",
        "        targeted=False,\n",
        "        learning_rate=0.01,\n",
        "        binary_search_steps=10,\n",
        "        max_iter=10,\n",
        "        initial_const=0.01,\n",
        "        mask=None,\n",
        "        #warm_start=True, # For later implementation of warm_start\n",
        "        max_halving=5,\n",
        "        max_doubling=5,\n",
        "        batch_size=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a Carlini L_0 attack instance.\n",
        "\n",
        "        :param classifier: A trained classifier.\n",
        "        :type classifier: :class:`.Classifier`\n",
        "        :param confidence: Confidence of adversarial examples: a higher value produces examples that are farther away,\n",
        "                from the original input, but classified with higher confidence as the target class.\n",
        "        :type confidence: `float`\n",
        "        :param targeted: Should the attack target one specific class.\n",
        "        :type targeted: `bool`\n",
        "        :param learning_rate: The initial learning rate for the attack algorithm. Smaller values produce better results\n",
        "                but are slower to converge.\n",
        "        :type learning_rate: `float`\n",
        "        :param binary_search_steps: Number of times to adjust constant with binary search (positive value). If\n",
        "                                    `binary_search_steps` is large, then the algorithm is not very sensitive to the\n",
        "                                    value of `initial_const`. Note that the values gamma=0.999999 and c_upper=10e10 are\n",
        "                                    hardcoded with the same values used by the authors of the method.\n",
        "        :type binary_search_steps: `int`\n",
        "        :param max_iter: The maximum number of iterations.\n",
        "        :type max_iter: `int`\n",
        "        :param initial_const: The initial trade-off constant `c` to use to tune the relative importance of distance and\n",
        "                confidence. If `binary_search_steps` is large, the initial constant is not important, as discussed in\n",
        "                Carlini and Wagner (2016).\n",
        "        :type initial_const: `float`\n",
        "        :param mask: The initial features that can be modified by the algorithm. If not specified, the\n",
        "                algorithm uses the full feature set.\n",
        "        :type mask: `np.ndarray`\n",
        "        :param warm_start: Instead of starting gradien descent in each iteration from the initial image. we start the\n",
        "                gradient descent from the solution found on the previous iteration.\n",
        "        :type warm_start: `boolean`\n",
        "        :param max_halving: Maximum number of halving steps in the line search optimization.\n",
        "        :type max_halving: `int`\n",
        "        :param max_doubling: Maximum number of doubling steps in the line search optimization.\n",
        "        :type max_doubling: `int`\n",
        "        :param batch_size: Size of the batch on which adversarial samples are generated.\n",
        "        :type batch_size: `int`\n",
        "        \"\"\"\n",
        "        super(CarliniL0Method, self).__init__(estimator=classifier)\n",
        "\n",
        "        kwargs = {\n",
        "            \"confidence\": confidence,\n",
        "            \"targeted\": targeted,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"binary_search_steps\": binary_search_steps,\n",
        "            \"max_iter\": max_iter,\n",
        "            \"initial_const\": initial_const,\n",
        "            \"mask\": mask,\n",
        "            #\"warm_start\": warm_start, # For later implementation of warm_start\n",
        "            \"max_halving\": max_halving,\n",
        "            \"max_doubling\": max_doubling,\n",
        "            \"batch_size\": batch_size,\n",
        "        }\n",
        "        assert self.set_params(**kwargs)\n",
        "\n",
        "        # There are internal hyperparameters:\n",
        "        # Abort binary search for c if it exceeds this threshold (suggested in Carlini and Wagner (2016)):\n",
        "        self._c_upper_bound = 10e10\n",
        "\n",
        "        # Smooth arguments of arctanh by multiplying with this constant to avoid division by zero.\n",
        "        # It appears this is what Carlini and Wagner (2016) are alluding to in their footnote 8. However, it is not\n",
        "        # clear how their proposed trick (\"instead of scaling by 1/2 we scale by 1/2 + eps\") works in detail.\n",
        "        self._tanh_smoother = 0.999999\n",
        "\n",
        "        # The tanh transformation does not always map inputs back to their original values event if they're unmodified\n",
        "        # To overcom this problem, we set a threshold of minimal diffrence considered as perturbation\n",
        "        # Below this threshold, a diffrence between values is considered as tanh transformation diffrence\n",
        "        self._perturbation_threshold = 1e-06\n",
        "\n",
        "    def _loss(self, x, x_adv, target, c_weight):\n",
        "        \"\"\"\n",
        "        Compute the objective function value.\n",
        "\n",
        "        :param x: An array with the original input.\n",
        "        :type x: `np.ndarray`\n",
        "        :param x_adv: An array with the adversarial input.\n",
        "        :type x_adv: `np.ndarray`\n",
        "        :param target: An array with the target class (one-hot encoded).\n",
        "        :type target: `np.ndarray`\n",
        "        :param c_weight: Weight of the loss term aiming for classification as target.\n",
        "        :type c_weight: `float`\n",
        "        :return: A tuple holding the current logits, l2 distance and overall loss.\n",
        "        :rtype: `(float, float, float)`\n",
        "        \"\"\"\n",
        "        l2dist = np.sum(np.square(x - x_adv).reshape(x.shape[0], -1), axis=1)\n",
        "        z_predicted = self.estimator.predict(\n",
        "            np.array(x_adv, dtype=ART_NUMPY_DTYPE), logits=True, batch_size=self.batch_size\n",
        "        )\n",
        "        z_target = np.sum(z_predicted * target, axis=1)\n",
        "        z_other = np.max(z_predicted * (1 - target) + (np.min(z_predicted, axis=1) - 1)[:, np.newaxis] * target, axis=1)\n",
        "\n",
        "        # The following differs from the exact definition given in Carlini and Wagner (2016). There (page 9, left\n",
        "        # column, last equation), the maximum is taken over Z_other - Z_target (or Z_target - Z_other respectively)\n",
        "        # and -confidence. However, it doesn't seem that that would have the desired effect (loss term is <= 0 if and\n",
        "        # only if the difference between the logit of the target and any other class differs by at least confidence).\n",
        "        # Hence the rearrangement here.\n",
        "\n",
        "        if self.targeted:\n",
        "            # if targeted, optimize for making the target class most likely\n",
        "            loss = np.maximum(z_other - z_target + self.confidence, np.zeros(x.shape[0]))\n",
        "        else:\n",
        "            # if untargeted, optimize for making any other class most likely\n",
        "            loss = np.maximum(z_target - z_other + self.confidence, np.zeros(x.shape[0]))\n",
        "\n",
        "        return z_predicted, l2dist, c_weight * loss + l2dist\n",
        "\n",
        "    def _loss_gradient(self, z_logits, target, x, x_adv, x_adv_tanh, c_weight, clip_min, clip_max):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function.\n",
        "\n",
        "        :param z_logits: An array with the current logits.\n",
        "        :type z_logits: `np.ndarray`\n",
        "        :param target: An array with the target class (one-hot encoded).\n",
        "        :type target: `np.ndarray`\n",
        "        :param x: An array with the original input.\n",
        "        :type x: `np.ndarray`\n",
        "        :param x_adv: An array with the adversarial input.\n",
        "        :type x_adv: `np.ndarray`\n",
        "        :param x_adv_tanh: An array with the adversarial input in tanh space.\n",
        "        :type x_adv_tanh: `np.ndarray`\n",
        "        :param c_weight: Weight of the loss term aiming for classification as target.\n",
        "        :type c_weight: `float`\n",
        "        :param clip_min: Minimum clipping value.\n",
        "        :type clip_min: `float`\n",
        "        :param clip_max: Maximum clipping value.\n",
        "        :type clip_max: `float`\n",
        "        :return: An array with the gradient of the loss function.\n",
        "        :type target: `np.ndarray`\n",
        "        \"\"\"\n",
        "        if self.targeted:\n",
        "            i_sub = np.argmax(target, axis=1)\n",
        "            i_add = np.argmax(z_logits * (1 - target) + (np.min(z_logits, axis=1) - 1)[:, np.newaxis] * target, axis=1)\n",
        "        else:\n",
        "            i_add = np.argmax(target, axis=1)\n",
        "            i_sub = np.argmax(z_logits * (1 - target) + (np.min(z_logits, axis=1) - 1)[:, np.newaxis] * target, axis=1)\n",
        "\n",
        "        loss_gradient = self.estimator.class_gradient(x_adv, label=i_add)\n",
        "        loss_gradient -= self.estimator.class_gradient(x_adv, label=i_sub)\n",
        "        loss_gradient = loss_gradient.reshape(x.shape)\n",
        "\n",
        "        c_mult = c_weight\n",
        "        for _ in range(len(x.shape) - 1):\n",
        "            c_mult = c_mult[:, np.newaxis]\n",
        "\n",
        "        loss_gradient *= c_mult\n",
        "        loss_gradient += 2 * (x_adv - x)\n",
        "        loss_gradient *= clip_max - clip_min\n",
        "        loss_gradient *= (1 - np.square(np.tanh(x_adv_tanh))) / (2 * self._tanh_smoother)\n",
        "\n",
        "        return loss_gradient\n",
        "\n",
        "\n",
        "    def generate(self, x, y=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Generate adversarial samples and return them in an array.\n",
        "\n",
        "        :param x: An array with the original inputs to be attacked.\n",
        "        :type x: `np.ndarray`\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,). If `self.targeted` is true, then `y` represents the target labels. If `self.targeted`\n",
        "                  is true, then `y_val` represents the target labels. Otherwise, the targets are the original class\n",
        "                  labels.\n",
        "        :return: An array holding the adversarial examples.\n",
        "        :rtype: `np.ndarray`\n",
        "        \"\"\"\n",
        "        y = check_and_transform_label_format(y, self.estimator.nb_classes)\n",
        "        x_adv = x.astype(ART_NUMPY_DTYPE)\n",
        "\n",
        "        if self.estimator.clip_values is not None:\n",
        "            clip_min, clip_max = self.estimator.clip_values\n",
        "        else:\n",
        "            clip_min, clip_max = np.amin(x), np.amax(x)\n",
        "\n",
        "        # Assert that, if attack is targeted, y_val is provided:\n",
        "        if self.targeted and y is None:\n",
        "            raise ValueError(\"Target labels `y` need to be provided for a targeted attack.\")\n",
        "\n",
        "        # No labels provided, use model prediction as correct class\n",
        "        if y is None:\n",
        "            y = get_labels_np_array(self.estimator.predict(x, batch_size=self.batch_size))\n",
        "\n",
        "        if self.mask is None:\n",
        "            # No initial activation provided, use the full feature set\n",
        "            activation = np.ones(x.shape)\n",
        "        else:\n",
        "            # Check if the initial activation has the same dimension as the input data\n",
        "            if self.mask.shape != x.shape:\n",
        "                raise ValueError(\"The mask must have the same dimensions as the input data.\")\n",
        "            activation = np.array(self.mask).astype(float)\n",
        "\n",
        "        # L_0 attack specific variables\n",
        "        final_adversarial_example = x.astype(ART_NUMPY_DTYPE)\n",
        "        old_activation = activation.copy()\n",
        "        c_final = np.ones(x.shape[0])\n",
        "        best_l0dist = np.inf * np.ones(x.shape[0])\n",
        "\n",
        "        # Main loop of the L_0 attack.\n",
        "        # For each iteration :\n",
        "        #   - Calls the L_2 attack to compute an adversarial example\n",
        "        #   - Computes the gradients of the objective function evaluated at the adversarial instance\n",
        "        #   - Fix the attribute with the lowest value (gradient * perturbation)\n",
        "        # Repeat until the L_2 attack fails to find an adversarial examples.\n",
        "        while True:\n",
        "            # Compute perturbation with implicit batching\n",
        "            nb_batches = int(np.ceil(x_adv.shape[0] / float(self.batch_size)))\n",
        "            for batch_id in range(nb_batches):\n",
        "                logger.debug(\"Processing batch %i out of %i\", batch_id, nb_batches)\n",
        "\n",
        "                batch_index_1, batch_index_2 = batch_id * self.batch_size, (batch_id + 1) * self.batch_size\n",
        "                x_batch = x[batch_index_1:batch_index_2]\n",
        "                #x_batch = x_adv[batch_index_1:batch_index_2] #use for future implementation of warm_start\n",
        "                y_batch = y[batch_index_1:batch_index_2]\n",
        "                activation_batch = activation[batch_index_1:batch_index_2]\n",
        "\n",
        "                # The optimization is performed in tanh space to keep the adversarial images bounded in correct range\n",
        "                x_batch_tanh = original_to_tanh(x_batch, clip_min, clip_max, self._tanh_smoother)\n",
        "\n",
        "                # Initialize binary search:\n",
        "                c_current = self.initial_const * np.ones(x_batch.shape[0])\n",
        "                c_lower_bound = np.zeros(x_batch.shape[0])\n",
        "                c_double = np.ones(x_batch.shape[0]) > 0\n",
        "\n",
        "                # Initialize placeholders for best l2 distance and attack found so far\n",
        "                best_l2dist = np.inf * np.ones(x_batch.shape[0])\n",
        "                best_l0dist_batch = np.inf * np.ones(x_batch.shape[0])\n",
        "                best_x_adv_batch = x_batch.copy()\n",
        "\n",
        "                for bss in range(self.binary_search_steps):\n",
        "                    logger.debug(\n",
        "                        \"Binary search step %i out of %i (c_mean==%f)\", bss, self.binary_search_steps, np.mean(c_current)\n",
        "                    )\n",
        "                    nb_active = int(np.sum(c_current < self._c_upper_bound))\n",
        "                    logger.debug(\n",
        "                        \"Number of samples with c_current < _c_upper_bound: %i out of %i\", nb_active, x_batch.shape[0]\n",
        "                    )\n",
        "                    if nb_active == 0:\n",
        "                        break\n",
        "                    learning_rate = self.learning_rate * np.ones(x_batch.shape[0])\n",
        "\n",
        "                    # Initialize perturbation in tanh space:\n",
        "                    x_adv_batch = x_batch.copy()\n",
        "                    x_adv_batch_tanh = x_batch_tanh.copy()\n",
        "\n",
        "                    z_logits, l2dist, loss = self._loss(x_batch, x_adv_batch, y_batch, c_current)\n",
        "                    attack_success = loss - l2dist <= 0\n",
        "                    overall_attack_success = attack_success\n",
        "\n",
        "                    for i_iter in range(self.max_iter):\n",
        "                        logger.debug(\"Iteration step %i out of %i\", i_iter, self.max_iter)\n",
        "                        logger.debug(\"Average Loss: %f\", np.mean(loss))\n",
        "                        logger.debug(\"Average L2Dist: %f\", np.mean(l2dist))\n",
        "                        logger.debug(\"Average Margin Loss: %f\", np.mean(loss - l2dist))\n",
        "                        logger.debug(\n",
        "                            \"Current number of succeeded attacks: %i out of %i\",\n",
        "                            int(np.sum(attack_success)),\n",
        "                            len(attack_success),\n",
        "                        )\n",
        "\n",
        "                        l0dist = np.sum((np.abs(x_batch - x_adv_batch) > self._perturbation_threshold).astype(int), axis=1)\n",
        "                        improved_adv = attack_success & (l0dist < best_l0dist_batch)\n",
        "                        logger.debug(\"Number of improved L0 distances: %i\", int(np.sum(improved_adv)))\n",
        "                        if np.sum(improved_adv) > 0:\n",
        "                            best_l0dist_batch[improved_adv] = l0dist[improved_adv]\n",
        "                            best_x_adv_batch[improved_adv] = x_adv_batch[improved_adv]\n",
        "\n",
        "                        active = (c_current < self._c_upper_bound) & (learning_rate > 0)\n",
        "                        nb_active = int(np.sum(active))\n",
        "                        logger.debug(\n",
        "                            \"Number of samples with c_current < _c_upper_bound and learning_rate > 0: %i out of %i\",\n",
        "                            nb_active,\n",
        "                            x_batch.shape[0],\n",
        "                        )\n",
        "                        if nb_active == 0:\n",
        "                            break\n",
        "\n",
        "                        # compute gradient:\n",
        "                        logger.debug(\"Compute loss gradient\")\n",
        "                        perturbation_tanh = -self._loss_gradient(\n",
        "                            z_logits[active],\n",
        "                            y_batch[active],\n",
        "                            x_batch[active],\n",
        "                            x_adv_batch[active],\n",
        "                            x_adv_batch_tanh[active],\n",
        "                            c_current[active],\n",
        "                            clip_min,\n",
        "                            clip_max,\n",
        "                        )\n",
        "\n",
        "                        # perform line search to optimize perturbation\n",
        "                        # first, halve the learning rate until perturbation actually decreases the loss:\n",
        "                        prev_loss = loss.copy()\n",
        "                        best_loss = loss.copy()\n",
        "                        best_lr = np.zeros(x_batch.shape[0])\n",
        "                        halving = np.zeros(x_batch.shape[0])\n",
        "\n",
        "                        for i_halve in range(self.max_halving):\n",
        "                            logger.debug(\"Perform halving iteration %i out of %i\", i_halve, self.max_halving)\n",
        "                            do_halving = loss[active] >= prev_loss[active]\n",
        "                            logger.debug(\"Halving to be performed on %i samples\", int(np.sum(do_halving)))\n",
        "                            if np.sum(do_halving) == 0:\n",
        "                                break\n",
        "                            active_and_do_halving = active.copy()\n",
        "                            active_and_do_halving[active] = do_halving\n",
        "\n",
        "                            lr_mult = learning_rate[active_and_do_halving]\n",
        "                            for _ in range(len(x.shape) - 1):\n",
        "                                lr_mult = lr_mult[:, np.newaxis]\n",
        "\n",
        "                            x_adv1 = x_adv_batch_tanh[active_and_do_halving]\n",
        "                            new_x_adv_batch_tanh = x_adv1 + lr_mult * perturbation_tanh[do_halving] * activation_batch[do_halving]\n",
        "                            new_x_adv_batch = tanh_to_original(new_x_adv_batch_tanh, clip_min, clip_max)\n",
        "                            _, l2dist[active_and_do_halving], loss[active_and_do_halving] = self._loss(\n",
        "                                x_batch[active_and_do_halving],\n",
        "                                new_x_adv_batch,\n",
        "                                y_batch[active_and_do_halving],\n",
        "                                c_current[active_and_do_halving],\n",
        "                            )\n",
        "\n",
        "                            logger.debug(\"New Average Loss: %f\", np.mean(loss))\n",
        "                            logger.debug(\"New Average L2Dist: %f\", np.mean(l2dist))\n",
        "                            logger.debug(\"New Average Margin Loss: %f\", np.mean(loss - l2dist))\n",
        "\n",
        "                            best_lr[loss < best_loss] = learning_rate[loss < best_loss]\n",
        "                            best_loss[loss < best_loss] = loss[loss < best_loss]\n",
        "                            learning_rate[active_and_do_halving] /= 2\n",
        "                            halving[active_and_do_halving] += 1\n",
        "                        learning_rate[active] *= 2\n",
        "\n",
        "                        # if no halving was actually required, double the learning rate as long as this\n",
        "                        # decreases the loss:\n",
        "                        for i_double in range(self.max_doubling):\n",
        "                            logger.debug(\"Perform doubling iteration %i out of %i\", i_double, self.max_doubling)\n",
        "                            do_doubling = (halving[active] == 1) & (loss[active] <= best_loss[active])\n",
        "                            logger.debug(\"Doubling to be performed on %i samples\", int(np.sum(do_doubling)))\n",
        "                            if np.sum(do_doubling) == 0:\n",
        "                                break\n",
        "                            active_and_do_doubling = active.copy()\n",
        "                            active_and_do_doubling[active] = do_doubling\n",
        "                            learning_rate[active_and_do_doubling] *= 2\n",
        "\n",
        "                            lr_mult = learning_rate[active_and_do_doubling]\n",
        "                            for _ in range(len(x.shape) - 1):\n",
        "                                lr_mult = lr_mult[:, np.newaxis]\n",
        "\n",
        "                            x_adv2 = x_adv_batch_tanh[active_and_do_doubling]\n",
        "                            new_x_adv_batch_tanh = x_adv2 + lr_mult * perturbation_tanh[do_doubling] * activation_batch[do_doubling]\n",
        "                            new_x_adv_batch = tanh_to_original(new_x_adv_batch_tanh, clip_min, clip_max)\n",
        "                            _, l2dist[active_and_do_doubling], loss[active_and_do_doubling] = self._loss(\n",
        "                                x_batch[active_and_do_doubling],\n",
        "                                new_x_adv_batch,\n",
        "                                y_batch[active_and_do_doubling],\n",
        "                                c_current[active_and_do_doubling],\n",
        "                            )\n",
        "                            logger.debug(\"New Average Loss: %f\", np.mean(loss))\n",
        "                            logger.debug(\"New Average L2Dist: %f\", np.mean(l2dist))\n",
        "                            logger.debug(\"New Average Margin Loss: %f\", np.mean(loss - l2dist))\n",
        "                            best_lr[loss < best_loss] = learning_rate[loss < best_loss]\n",
        "                            best_loss[loss < best_loss] = loss[loss < best_loss]\n",
        "\n",
        "                        learning_rate[halving == 1] /= 2\n",
        "\n",
        "                        update_adv = best_lr[active] > 0\n",
        "                        logger.debug(\"Number of adversarial samples to be finally updated: %i\", int(np.sum(update_adv)))\n",
        "\n",
        "                        if np.sum(update_adv) > 0:\n",
        "                            active_and_update_adv = active.copy()\n",
        "                            active_and_update_adv[active] = update_adv\n",
        "                            best_lr_mult = best_lr[active_and_update_adv]\n",
        "                            for _ in range(len(x.shape) - 1):\n",
        "                                best_lr_mult = best_lr_mult[:, np.newaxis]\n",
        "\n",
        "                            x_adv4 = x_adv_batch_tanh[active_and_update_adv]\n",
        "                            best_lr1 = best_lr_mult * perturbation_tanh[update_adv]\n",
        "                            x_adv_batch_tanh[active_and_update_adv] = x_adv4 + best_lr1 * activation_batch[active_and_update_adv]\n",
        "\n",
        "                            x_adv6 = x_adv_batch_tanh[active_and_update_adv]\n",
        "                            x_adv_batch[active_and_update_adv] = tanh_to_original(x_adv6, clip_min, clip_max)\n",
        "                            (\n",
        "                                z_logits[active_and_update_adv],\n",
        "                                l2dist[active_and_update_adv],\n",
        "                                loss[active_and_update_adv],\n",
        "                            ) = self._loss(\n",
        "                                x_batch[active_and_update_adv],\n",
        "                                x_adv_batch[active_and_update_adv],\n",
        "                                y_batch[active_and_update_adv],\n",
        "                                c_current[active_and_update_adv],\n",
        "                            )\n",
        "                            attack_success = loss - l2dist <= 0\n",
        "                            overall_attack_success = overall_attack_success | attack_success\n",
        "\n",
        "                    # Update depending on attack success:\n",
        "                    l0dist = np.sum((np.abs(x_batch - x_adv_batch) > self._perturbation_threshold).astype(int), axis=1)\n",
        "                    improved_adv = attack_success & (l0dist < best_l0dist_batch)\n",
        "                    logger.debug(\"Number of improved L0 distances: %i\", int(np.sum(improved_adv)))\n",
        "                    if np.sum(improved_adv) > 0:\n",
        "                        best_l0dist_batch[improved_adv] = l0dist[improved_adv]\n",
        "                        best_x_adv_batch[improved_adv] = x_adv_batch[improved_adv]\n",
        "\n",
        "                    c_double[overall_attack_success] = False\n",
        "                    c_current[overall_attack_success] = (c_lower_bound + c_current)[overall_attack_success] / 2\n",
        "\n",
        "                    c_old = c_current\n",
        "                    c_current[~overall_attack_success & c_double] *= 2\n",
        "\n",
        "                    c_current1 = (c_current - c_lower_bound)[~overall_attack_success & ~c_double]\n",
        "                    c_current[~overall_attack_success & ~c_double] += c_current1 / 2\n",
        "                    c_lower_bound[~overall_attack_success] = c_old[~overall_attack_success]\n",
        "\n",
        "                c_final[batch_index_1:batch_index_2] = c_current\n",
        "                x_adv[batch_index_1:batch_index_2] = best_x_adv_batch\n",
        "\n",
        "            logger.info(\n",
        "                \"Success rate of C&W L_2 attack: %.2f%%\",\n",
        "                100 * compute_success(self.estimator, x, y, x_adv, self.targeted, batch_size=self.batch_size),\n",
        "            )\n",
        "\n",
        "            # If the L_2 attack can't find any adversarial examples with the new activation, return the last one\n",
        "            z_logits, l2dist, loss = self._loss(x, x_adv, y, c_final)\n",
        "            attack_success = loss - l2dist <= 0\n",
        "            #l0dist = np.sum((np.abs(x_batch - x_adv_batch) > self._perturbation_threshold).astype(int), axis=1)\n",
        "            l0dist = np.sum((np.abs(x - x_adv) > self._perturbation_threshold).astype(int), axis=1)\n",
        "            improved_adv = attack_success & (l0dist < best_l0dist)\n",
        "            if np.sum(improved_adv) > 0:\n",
        "                final_adversarial_example[improved_adv] = x_adv[improved_adv]\n",
        "            else:\n",
        "                return x*(old_activation == 0).astype(int) + final_adversarial_example*old_activation\n",
        "\n",
        "            # Compute the gradients of the objective function evaluated at the adversarial instance\n",
        "            x_adv_tanh = original_to_tanh(x_adv, clip_min, clip_max, self._tanh_smoother)\n",
        "            objective_loss_gradient = -self._loss_gradient(\n",
        "                z_logits,\n",
        "                y,\n",
        "                x,\n",
        "                x_adv,\n",
        "                x_adv_tanh,\n",
        "                c_final,\n",
        "                clip_min,\n",
        "                clip_max,\n",
        "            )\n",
        "            perturbation_L1_norm = np.abs(x_adv - x)\n",
        "\n",
        "            # gradient * perturbation tells how much reduction to the objective function we obtain for each attribute\n",
        "            objective_reduction = np.abs(objective_loss_gradient) * perturbation_L1_norm\n",
        "\n",
        "            # Put a huge nomber as objective_reduction value for fixed feature (in order not to select them again)\n",
        "            # was np.inf before, but inf * 0 = nan\n",
        "            objective_reduction += 999999999999999 * (activation == 0).astype(int)\n",
        "\n",
        "            # Fix the feature with the lowest objective_reduction value (only for the examples that succeeded)\n",
        "            fix_feature_index = np.argmin(objective_reduction, axis=1)\n",
        "            fix_feature = np.ones(x.shape)\n",
        "            fix_feature[np.arange(fix_feature_index.size), fix_feature_index] = 0\n",
        "            old_activation[improved_adv]  = activation.copy()[improved_adv]\n",
        "            activation[improved_adv]  *= fix_feature[improved_adv]\n",
        "            print(\n",
        "                \"L0 norm before fixing :\\n{}\\nNumber active features :\\n{}\\nIndex of fixed feature :\\n{}\"\n",
        "                .format(\n",
        "                    np.sum((perturbation_L1_norm > self._perturbation_threshold).astype(int), axis=1),\n",
        "                    np.sum(activation, axis=1),\n",
        "                    fix_feature_index\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def set_params(self, **kwargs):\n",
        "        \"\"\"Take in a dictionary of parameters and applies attack-specific checks before saving them as attributes.\n",
        "\n",
        "        :param confidence: Confidence of adversarial examples: a higher value produces examples that are farther away,\n",
        "               from the original input, but classified with higher confidence as the target class.\n",
        "        :type confidence: `float`\n",
        "        :param targeted: Should the attack target one specific class\n",
        "        :type targeted: `bool`\n",
        "        :param learning_rate: The learning rate for the attack algorithm. Smaller values produce better results but are\n",
        "               slower to converge.\n",
        "        :type learning_rate: `float`\n",
        "        :param binary_search_steps: number of times to adjust constant with binary search (positive value)\n",
        "        :type binary_search_steps: `int`\n",
        "        :param max_iter: The maximum number of iterations.\n",
        "        :type max_iter: `int`\n",
        "        :param initial_const: (optional float, positive) The initial trade-off constant c to use to tune the relative\n",
        "               importance of distance and confidence. If binary_search_steps is large,\n",
        "               the initial constant is not important. The default value 1e-4 is suggested in Carlini and Wagner (2016).\n",
        "        :type initial_const: `float`\n",
        "        :param max_halving: Maximum number of halving steps in the line search optimization.\n",
        "        :type max_halving: `int`\n",
        "        :param max_doubling: Maximum number of doubling steps in the line search optimization.\n",
        "        :type max_doubling: `int`\n",
        "        :param batch_size: Internal size of batches on which adversarial samples are generated.\n",
        "        :type batch_size: `int`\n",
        "        \"\"\"\n",
        "        # Save attack-specific parameters\n",
        "        super(CarliniL0Method, self).set_params(**kwargs)\n",
        "\n",
        "        if not isinstance(self.binary_search_steps, (int, np.int)) or self.binary_search_steps < 0:\n",
        "            raise ValueError(\"The number of binary search steps must be a non-negative integer.\")\n",
        "\n",
        "        if not isinstance(self.max_iter, (int, np.int)) or self.max_iter < 0:\n",
        "            raise ValueError(\"The number of iterations must be a non-negative integer.\")\n",
        "\n",
        "        if not isinstance(self.max_halving, (int, np.int)) or self.max_halving < 1:\n",
        "            raise ValueError(\"The number of halving steps must be an integer greater than zero.\")\n",
        "\n",
        "        if not isinstance(self.max_doubling, (int, np.int)) or self.max_doubling < 1:\n",
        "            raise ValueError(\"The number of doubling steps must be an integer greater than zero.\")\n",
        "\n",
        "        if not isinstance(self.batch_size, (int, np.int)) or self.batch_size < 1:\n",
        "            raise ValueError(\"The batch size must be an integer greater than zero.\")\n",
        "\n",
        "        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}